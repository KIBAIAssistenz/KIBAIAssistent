# # UI.py
# import os
# import gradio as gr

# from services.llm_connector import llm

# from experts.einf√ºhrung_KI.expert_einf√ºhrung_KI import build_einf√ºhrung_KI_expert
# from experts.machine_learning.expert_ml import build_machine_learning_expert

# os.environ.setdefault("GRADIO_ANALYTICS_ENABLED", "False")

# EXPERT_FACTORIES = {
#     "Einf√ºhrung in die KI": lambda: build_einf√ºhrung_KI_expert(llm),
#     "Machine Learning":     lambda: build_machine_learning_expert(llm),
# }
# EXPERT_CACHE = {}
# DEFAULT_MODULE = "Einf√ºhrung in die KI"

# def get_expert(label: str):
#     if label in EXPERT_CACHE:
#         return EXPERT_CACHE[label]
#     print(f"[UI] Baue Experte '{label}' ...")
#     exp = EXPERT_FACTORIES[label]()   # kann beim ersten Mal l√§nger dauern
#     EXPERT_CACHE[label] = exp
#     print(f"[UI] Experte '{label}' bereit.")
#     return exp

# def answer(message, history, module_label: str):
#     try:
#         exp = get_expert(module_label)
#     except Exception as e:
#         return f"‚ö†Ô∏è Konnte den Experten '{module_label}' nicht laden: {e}"
#     try:
#         # Wenn deine Chain History braucht: exp["chain"].invoke({"question": message, "history": history})
#         return exp["chain"].invoke(message)
#     except Exception as e:
#         return f"‚ö†Ô∏è Fehler bei der Anfrage: {e}"

# def build_app():
#     with gr.Blocks(title="BAI Lernassistent") as demo:
#         gr.Markdown("# üìò W√§hle dein Modul")

#         module_radio = gr.Radio(
#             choices=list(EXPERT_FACTORIES.keys()),
#             value=DEFAULT_MODULE,
#             label="Modul",
#             info="Du kannst jederzeit wechseln."
#         )
#         info = gr.Markdown(f"‚úÖ **{DEFAULT_MODULE}** gew√§hlt.")
#         preload_btn = gr.Button("üîÑ Modul vorladen (optional)")

#         chat = gr.ChatInterface(
#             fn=answer,
#             additional_inputs=[module_radio],
#             type="messages",                      # falls das bei dir Probleme macht, entferne die Zeile
#             title="Chat zum gew√§hlten Modul",
#             textbox=gr.Textbox(placeholder="Stelle deine Frage zum gew√§hlten Modul ‚Ä¶"),
#         )

#         def on_module_change(sel):
#             return gr.update(value=f"‚úÖ **{sel}** gew√§hlt.")
#         module_radio.change(on_module_change, inputs=module_radio, outputs=info)

#         def preload(sel):
#             try:
#                 get_expert(sel)
#                 return gr.update(value=f"‚úÖ **{sel}** geladen und bereit.")
#             except Exception as e:
#                 return gr.update(value=f"‚ùå Fehler beim Laden von **{sel}**: {e}")
#         preload_btn.click(preload, inputs=module_radio, outputs=info)

#     return demo

# if __name__ == "__main__":
#     print("[UI] baue App ‚Ä¶")
#     app = build_app()
#     print("[UI] starte Gradio ‚Ä¶")
#     app.launch(
#         inbrowser=True,
#         server_name="127.0.0.1",
#         server_port=7860,   # bei Konflikt eine andere Zahl nehmen, z.B. 7861
#         show_error=True,
#     )

# UI.py

# app/UI.py

# app/UI.py

import os
import sys
import pathlib

# üîß Pfad fixen: gehe eine Ebene √ºber /app ‚Üí Projektwurzel (KIBAIAssistent-1)
sys.path.insert(0, str(pathlib.Path(__file__).resolve().parents[1]))

import gradio as gr

# LLM / Services
from services.llm_connector import llm

# Experten
from experts.einf√ºhrung_KI.expert_einf√ºhrung_KI import build_einf√ºhrung_KI_expert
from experts.machine_learning.expert_ml import build_machine_learning_expert

# Router + Web-Tools
from experts.router import answer_with_module_and_web_fallback
from services.tools.tool_web_einfuehrung_ki import ki_web_search
from services.tools.tool_web_ml import ml_web_search

os.environ.setdefault("GRADIO_ANALYTICS_ENABLED", "False")

# =========================================
# Experten-Definition & Caching
# =========================================

EXPERT_FACTORIES = {
    "Einf√ºhrung in die KI": lambda: build_einf√ºhrung_KI_expert(llm),
    "Machine Learning":     lambda: build_machine_learning_expert(llm),
}
EXPERT_CACHE = {}
DEFAULT_MODULE = "Einf√ºhrung in die KI"

# Web-Tools passend zu den Modulen
WEB_TOOLS = {
    "Einf√ºhrung in die KI": ki_web_search,
    "Machine Learning":     ml_web_search,
}


def get_expert(label: str):
    """L√§dt (und cached) den Experten f√ºr das gew√§hlte Modul."""
    if label in EXPERT_CACHE:
        return EXPERT_CACHE[label]

    print(f"[UI] Baue Experte '{label}' ...")
    exp = EXPERT_FACTORIES[label]()   # kann beim ersten Mal l√§nger dauern
    EXPERT_CACHE[label] = exp
    print(f"[UI] Experte '{label}' bereit.")
    return exp


# =========================================
# Antwort-Funktion f√ºr den Chat
# =========================================

def answer(message, history, module_label: str):
    """
    message: letzte User-Nachricht (String)
    history: bisheriger Verlauf (Liste von (user, assistant)-Tupeln)
    module_label: aktuell gew√§hltes Modul (z.B. 'Einf√ºhrung in die KI')
    """
    print(f"[UI] Frage: {message} | Modul: {module_label}")

    # Alle Experten-Objekte (mit Cache) bereitstellen
    try:
        experts = {label: get_expert(label) for label in EXPERT_FACTORIES.keys()}
    except Exception as e:
        return f"‚ö†Ô∏è Konnte die Experten nicht laden: {e}"

    try:
        result = answer_with_module_and_web_fallback(
            active_expert_name=module_label,
            experts=experts,
            web_tools=WEB_TOOLS,
            question=message,
            history=history,
        )

        answer_text = result["answer"]
        source_type = result.get("source_type", "unknown")
        print(f"[UI] source_type = {source_type}")

        return answer_text

    except Exception as e:
        print(f"[UI] Fehler in answer(): {e}")
        return f"‚ö†Ô∏è Fehler bei der Anfrage: {e}"


# =========================================
# Gradio-App
# =========================================

def build_app():
    with gr.Blocks(title="BAI Lernassistent") as demo:
        gr.Markdown("# üìò W√§hle dein Modul")

        module_radio = gr.Radio(
            choices=list(EXPERT_FACTORIES.keys()),
            value=DEFAULT_MODULE,
            label="Modul",
            info="Du kannst jederzeit wechseln."
        )
        info = gr.Markdown(f"‚úÖ **{DEFAULT_MODULE}** gew√§hlt.")
        preload_btn = gr.Button("üîÑ Modul vorladen (optional)")

        chat = gr.ChatInterface(
            fn=answer,
            additional_inputs=[module_radio],
            # type='tuples' ‚Üí message: str, history: Liste[(user, assistant)]
            type="tuples",
            title="Chat zum gew√§hlten Modul",
            textbox=gr.Textbox(placeholder="Stelle deine Frage zum gew√§hlten Modul ‚Ä¶"),
        )

        def on_module_change(sel):
            return gr.update(value=f"‚úÖ **{sel}** gew√§hlt.")
        module_radio.change(on_module_change, inputs=module_radio, outputs=info)

        def preload(sel):
            try:
                get_expert(sel)
                return gr.update(value=f"‚úÖ **{sel}** geladen und bereit.")
            except Exception as e:
                return gr.update(value=f"‚ùå Fehler beim Laden von **{sel}**: {e}")
        preload_btn.click(preload, inputs=module_radio, outputs=info)

    return demo


if __name__ == "__main__":
    print("[UI] baue App ‚Ä¶")
    app = build_app()
    print("[UI] starte Gradio ‚Ä¶")
    app.launch(
        inbrowser=True,
        server_name="127.0.0.1",
        server_port=7860,   # bei Konflikt eine andere Zahl nehmen, z.B. 7861
        show_error=True,
    )
